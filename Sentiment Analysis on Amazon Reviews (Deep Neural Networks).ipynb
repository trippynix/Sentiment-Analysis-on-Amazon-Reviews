{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2589bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data = pd.read_csv(\"train_preprocessed.csv\")\n",
    "test_data = pd.read_csv(\"test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbd1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 359979\n",
      "Length of test data: 39997\n"
     ]
    }
   ],
   "source": [
    "# Reducing the data as my machine cannot handle big data even with gpu acceleration.\n",
    "train_data_cut = len(train_data)//10\n",
    "test_data_cut = len(test_data)//10\n",
    "train_data = train_data[:train_data_cut]\n",
    "test_data = test_data[:test_data_cut]\n",
    "\n",
    "print(f\"Length of train data: {len(train_data)}\")\n",
    "print(f\"Length of test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed614748",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "max_words = 0\n",
    "min_words = 1000\n",
    "for i in range(len(train_data)):\n",
    "    max_words = max(max_words, len(train_data['Text'].iloc[i].split())) # checking max no. of words\n",
    "    min_words = min(min_words, len(train_data['Text'].iloc[i].split())) # checking min no. of words\n",
    "    \n",
    "    count += len(train_data['Text'].iloc[i].split())\n",
    "    # Calculating average no. of words in 2 diff ways\n",
    "    average_words1 = (max_words + min_words)/2 \n",
    "    average_words2 = count/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200939ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words:240\n",
      "Minimum number of words:5\n",
      "Average number of words if averaged max and min numbers:122.5\n",
      "Total words (just for fun):27198819\n",
      "Average number of words if averaged total words by the len of data:75.55668247314426\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum number of words:{max_words}\")\n",
    "print(f\"Minimum number of words:{min_words}\")\n",
    "print(f\"Average number of words if averaged max and min numbers:{average_words1}\")\n",
    "print(f\"Total words (just for fun):{count}\")\n",
    "print(f\"Average number of words if averaged total words by the len of data:{average_words2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17776b5",
   "metadata": {},
   "source": [
    "### I will be using average_words1 just because it has a bigger number obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd4bbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rounding up the average words\n",
    "average_words = round(average_words1)\n",
    "average_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31436149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Unique words in the dataset\n",
    "import re\n",
    "def unique_words(train_sentence):\n",
    "    train_sentence = train_sentence.lower()\n",
    "    train_sentence = re.sub(r'[^\\w\\s]', '', train_sentence)\n",
    "    words = train_sentence.split()\n",
    "    unique_words_set = set(words)\n",
    "    return unique_words_set\n",
    "\n",
    "uniqueWords = set()\n",
    "for i in range(len(train_data)):\n",
    "    uniqueWords.update(unique_words(train_data['Text'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ff09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:433193\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Unique Words:{len(uniqueWords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9291ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer_int = TextVectorization(max_tokens = 10000,\n",
    "                                   standardize = \"lower_and_strip_punctuation\",\n",
    "                                   split = \"whitespace\",\n",
    "                                   output_mode = \"int\",\n",
    "                                   output_sequence_length = 128)\n",
    "vectorizer_tfidf = TextVectorization(max_tokens = 10000,\n",
    "                                     standardize = \"lower_and_strip_punctuation\",\n",
    "                                     split = \"whitespace\",\n",
    "                                     output_mode = \"tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c69457",
   "metadata": {},
   "source": [
    "### After running the fit function I got to know that the model requires it's target to be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0b5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Positive to 1 and Negative to 0\n",
    "train_data['Polarity'] = train_data['Polarity'].map({'Positive': 1, 'Negative': 0})\n",
    "test_data['Polarity'] = test_data['Polarity'].map({'Positive': 1, 'Negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c75531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation feature data:\n",
      "0        Just plain cute... Hugh Jackman is so handsome...\n",
      "1        This is a good reason why people download musi...\n",
      "2        Good Value This was ordered for a teenager who...\n",
      "3        Doesn't even play I think it's not an authenti...\n",
      "4        It will become a part of you... Sweet Dream Ba...\n",
      "                               ...                        \n",
      "35992    THIS GAME SUCKS!! I mean it is really sweet to...\n",
      "35993    Prehistoric Research Fiction? I can't say enou...\n",
      "35994    AWFUL!!! THESE DIAPERS ARE HORRIBLE. I ALWAYS ...\n",
      "35995    This book is a wast of good paper Oh man, wher...\n",
      "35996    I couldn't put it down!! A set of wonderful st...\n",
      "Name: Feature, Length: 35997, dtype: object\n",
      "Valdiation target data:\n",
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "35992    0\n",
      "35993    1\n",
      "35994    0\n",
      "35995    0\n",
      "35996    1\n",
      "Name: Polarity, Length: 35997, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combining Title and Text columns into a single column\n",
    "train_data['Feature'] = train_data['Title'] + ' ' + train_data['Text']\n",
    "test_data['Feature'] = test_data['Title'] + ' ' + test_data['Text']\n",
    "\n",
    "# Splitting the data into features and target\n",
    "train_features = train_data['Feature']\n",
    "train_target = train_data['Polarity']\n",
    "test_features = test_data['Feature']\n",
    "test_target = test_data['Polarity']\n",
    "\n",
    "# Making validation set\n",
    "val_features = train_features[:int((0.1)*len(train_features))].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_target = train_target[:int((0.1)*len(train_target))].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Validation feature data:\\n{val_features}\")\n",
    "print(f\"Valdiation target data:\\n{val_target}\")\n",
    "\n",
    "# Converting to numpy arr for better processing in tf\n",
    "train_features = train_features.to_numpy()\n",
    "train_target = train_target.to_numpy()\n",
    "test_features = test_features.to_numpy()\n",
    "test_target = test_target.to_numpy()\n",
    "val_features = val_features.to_numpy()\n",
    "val_target = val_target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c21ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting the vectorizer to train_features\n",
    "vectorizer_int.adapt(train_features)\n",
    "vectorizer_tfidf.adapt(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebed896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer int:\n",
      "[[123 110  44  15 141   8   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]]\n",
      "\n",
      "Vectorizer tfidf:\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = 'Bad product. Would not recommend it'\n",
    "a = vectorizer_int([sample])\n",
    "b = vectorizer_tfidf([sample])\n",
    "print(f\"Vectorizer int:\\n{a}\\n\")\n",
    "print(f\"Vectorizer tfidf:\\n{b}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5f623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words:\n",
      "['', '[UNK]', 'the', 'and', 'a', 'i', 'to', 'of', 'it', 'this']\n",
      "Bottom 10 words:\n",
      "['brewed', 'breathless', 'brando', 'bookthis', 'bonnie', 'blunt', 'behaviors', 'appendix', 'amber', 'adjectives']\n"
     ]
    }
   ],
   "source": [
    "words_in_vocab = vectorizer_int.get_vocabulary()\n",
    "print(f\"Top 10 words:\\n{words_in_vocab[:10]}\")\n",
    "print(f\"Bottom 10 words:\\n{words_in_vocab[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17b2d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique words in vocab: {len(words_in_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b779ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = Embedding(input_dim = len(words_in_vocab),\n",
    "                      output_dim = 128,\n",
    "                      input_length = average_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b8acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "Bad product. Would not recommend it\n",
      "Embedded sentence of 'int' vector:\n",
      "[[[-0.03824536  0.01471433 -0.03776103 ... -0.00113573  0.01162968\n",
      "   -0.02690892]\n",
      "  [ 0.03346907 -0.00536389  0.04927898 ...  0.01422555 -0.01565919\n",
      "   -0.02491096]\n",
      "  [ 0.0311914   0.02994764  0.01037955 ...  0.0243897   0.02777798\n",
      "    0.03203355]\n",
      "  ...\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]]]\n",
      "Embedded sentence of 'tfidf' vector:\n",
      "[[[ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  ...\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]\n",
      "  [ 0.00529462  0.03227958 -0.04427711 ...  0.00457172 -0.03220586\n",
      "   -0.04457368]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original sentence:\\n{sample}\")\n",
    "sample_embed_int = embedding(vectorizer_int([sample]))\n",
    "sample_embed_tfidf = embedding(vectorizer_tfidf([sample]))\n",
    "print(f\"Embedded sentence of 'int' vector:\\n{sample_embed_int}\")\n",
    "print(f\"Embedded sentence of 'tfidf' vector:\\n{sample_embed_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5bb29",
   "metadata": {},
   "source": [
    "### Training deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b98ada96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training GRU and LSTM layer stacked model\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
    "x = vectorizer_int(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(128, return_sequences = True)(x)\n",
    "x = layers.LSTM(128, return_sequences = True)(x)\n",
    "x = layers.GRU(64)(x)\n",
    "x = layers.Dense(64, activation = 'relu')(x)\n",
    "outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "RNN1 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd91f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 128)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 128)          1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128, 128)          99072     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128, 128)          131584    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,552,129\n",
      "Trainable params: 1,552,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bc1639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "RNN1.compile(loss = 'binary_crossentropy',\n",
    "            optimizer = tf.keras.optimizers.Adam(),\n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f96525e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11250/11250 [==============================] - 570s 49ms/step - loss: 0.2401 - accuracy: 0.8903 - val_loss: 0.1481 - val_accuracy: 0.9451\n",
      "Epoch 2/5\n",
      "11250/11250 [==============================] - 544s 48ms/step - loss: 0.1543 - accuracy: 0.9417 - val_loss: 0.1183 - val_accuracy: 0.9587\n",
      "Epoch 3/5\n",
      "11250/11250 [==============================] - 543s 48ms/step - loss: 0.1283 - accuracy: 0.9532 - val_loss: 0.0951 - val_accuracy: 0.9674\n",
      "Epoch 4/5\n",
      "11250/11250 [==============================] - 541s 48ms/step - loss: 0.1060 - accuracy: 0.9624 - val_loss: 0.0789 - val_accuracy: 0.9768\n",
      "Epoch 5/5\n",
      "11250/11250 [==============================] - 540s 48ms/step - loss: 0.0864 - accuracy: 0.9701 - val_loss: 0.0620 - val_accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "RNN_model_intVect = RNN1.fit(train_features,\n",
    "                    train_target,\n",
    "                    epochs = 5,\n",
    "                    validation_data = (val_features, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3ff090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 29s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9980615 ],\n",
       "       [0.9994343 ],\n",
       "       [0.00162119],\n",
       "       [0.22521397],\n",
       "       [0.9957461 ],\n",
       "       [0.00746008],\n",
       "       [0.00326221],\n",
       "       [0.03149999],\n",
       "       [0.9908599 ],\n",
       "       [0.0250385 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_model_intVect_pred = RNN1.predict(test_features)\n",
    "RNN_model_intVect_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7f6935d-4817-496d-a8b8-f30382b7e505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 0., 0., 1., 0., 0., 0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting RNN1 predictions to labels\n",
    "RNN_model_intVect_pred = tf.squeeze(tf.round(RNN_model_intVect_pred))\n",
    "RNN_model_intVect_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c39882d-6d56-451e-a7e8-d55ac328efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a helper function to evaluate accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def evaluationMetrics(y_true, y_pred):\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average = 'weighted')\n",
    "    \n",
    "    model_result = {\"Accuracy\": model_accuracy,\n",
    "                    \"Precision\": model_precision,\n",
    "                    \"Recall\": model_recall,\n",
    "                    \"F1-Score\": model_f1}\n",
    "    \n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6d18d05-3c57-4951-8e3e-0355eda9a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the model on unseen data: {'Accuracy': 93.16948771157837, 'Precision': 0.9316955269754872, 'Recall': 0.9316948771157837, 'F1-Score': 0.9316926641262752}\n"
     ]
    }
   ],
   "source": [
    "RNN1_eval = evaluationMetrics(test_target, RNN_model_intVect_pred)\n",
    "print(f\"Result of the model on unseen data: {RNN1_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8921a27-4703-45c9-af91-359c978a1116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: RNN_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: RNN_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model in TensorFlow's SavedModel format\n",
    "RNN1.save('RNN_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a74bc-2115-46cf-9a09-f83dee60faf5",
   "metadata": {},
   "source": [
    "# Looks like even after training the model on 10% of original data it outperformed the NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df970e55-4599-4857-83de-9e6d6a77e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "RNN1 = load_model('RNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f963d635-4bbe-47a0-aec3-020d4ddb242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 300s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Let's predict on the original test data\n",
    "test_data_og = pd.read_csv(\"test_preprocessed.csv\")\n",
    "test_data_og['Feature'] = test_data_og['Title'] + ' ' + test_data_og['Text']\n",
    "test_features_og = test_data_og['Feature']\n",
    "\n",
    "RNN_model_intVect_pred_og = RNN1.predict(test_features_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c608cb4-6759-4822-b4f7-2ca97d9ea363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the model on unseen original data: {'Accuracy': 93.10683641018461, 'Precision': 0.9310812328837387, 'Recall': 0.9310683641018461, 'F1-Score': 0.9310678455866469}\n"
     ]
    }
   ],
   "source": [
    "# Evaluating \n",
    "import tensorflow as tf\n",
    "RNN_model_intVect_pred_og = tf.squeeze(tf.round(RNN_model_intVect_pred_og))\n",
    "test_data_og['Polarity'] = test_data_og['Polarity'].map({'Positive': 1, 'Negative': 0})\n",
    "test_target_og = test_data_og['Polarity']\n",
    "\n",
    "RNN1_eval_og = evaluationMetrics(test_target_og, RNN_model_intVect_pred_og)\n",
    "print(f\"Result of the model on unseen original data: {RNN1_eval_og}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03eab7-5da0-4246-b57c-00f392bc7435",
   "metadata": {},
   "source": [
    "# The model turned out to be incredible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f582734-685a-477a-86c8-e7287abe2d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
